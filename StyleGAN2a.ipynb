{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StyleGAN2-ada operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules for notebook logging\n",
    "!pip install IPython\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's prepare the data. Ensure all your images have the same color channels (monochrome, RGB or RGBA).  \n",
    "\n",
    "If you work with patterns or shapes (rather than compostions), you can crop square fragments from bigger images (effectively multiplying their amount). For that, edit source and target paths below; `size` is fragment resolution, `step` is shift between the fragments. This will cut every source image into 512x512px fragments, overlapped with 256px shift by X and Y. Edit `size` and `overlap`  according to your dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src_dir = 'data/src'\n",
    "data_dir = 'data/mydata'\n",
    "size = 512\n",
    "step = 256\n",
    "\n",
    "%run src/util/multicrop.py --in_dir $src_dir --out_dir $data_dir --size $size --step $step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you edit the images yourself (e.g. for non-square aspect ratios), ensure their correct size and put to the `data_dir`.  \n",
    "For conditional model split the data by subfolders (`mydata/1`, `mydata/2`, ..) and add `--cond` option to the training command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/mydata'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can train StyleGAN2 on the prepared dataset (which is the image folder itself, no need to convert it to some special format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/train.py --data $data_dir --kimg 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This will run training process, according to the options and settings in `src/train.py` (check and explore those!!). Results (models and samples) are saved under `train` directory, similar to original Nvidia approach. There are two types of models saved: compact (containing only Gs network for inference) as `<dataset>-...pkl` (e.g. `mydata-512-0360.pkl`), and full (containing G/D/Gs networks for further training) as `snapshot-...pkl`. \n",
    "\n",
    "> The length of the training is defined by `--kimg X` argument (training duration in thousands of images). Reasonable `kimg` value for full training from scratch is 5000-8000, while for finetuning in `--d_aug` mode 1000-2000 may be sufficient.\n",
    "\n",
    "> NB: If you get `RuntimeError: context has already been set` (probably after training interruption), restart IPython kernel and try again.  \n",
    "If you have troubles with custom cuda ops, try removing their cached version (`C:\\Users\\eps\\AppData\\Local\\torch_extensions` on Windows).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the training process was interrupted, we can resume it from the last saved model as following:  \n",
    "*(replace `000-mydata-512-...` with existing training directory)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/train.py --data $data_dir --resume train/001-mydata-512-auto-gf_bnc --kimg 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: In most cases it's much easier to use a \"transfer learning\" trick, rather than perform full training from the scratch. For that, we use existing well-trained model as a starter, and \"finetune\" (uptrain) it with our data. This works pretty well, even if our dataset is very different from the original model. \n",
    "\n",
    "So here is a faster way to train our GAN (presuming we have `ffhq-512.pkl` model already):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/train.py --data $data_dir --resume train/ffhq-512.pkl --kimg 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "Let's produce some imagery from the original cat model (download it from [here](https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-cat-config-f.pkl) and put to `models` directory).  \n",
    "More cool models can be found [here](https://github.com/justinpinkney/awesome-pretrained-stylegan2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, Video\n",
    "\n",
    "model = 'models/stylegan2-cat-config-f' # without \".pkl\" extension\n",
    "model_pkl = model + '.pkl' # with \".pkl\" extension\n",
    "output = '_out/cats'\n",
    "frames = '50-10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some animation to test the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/_genSGAN2.py --model $model_pkl --out_dir $output --frames $frames\n",
    "\n",
    "Image(filename = output + '/000000.jpg') # show first frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlzqD3DEGzIH"
   },
   "source": [
    "> Here we loaded the model 'as is', and produced 50 frames in its native resolution, interpolating between random latent space keypoints, with a step of 10 frames between keypoints.\n",
    "\n",
    "If you have `ffmpeg` installed, you can convert it into video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sequence = output + '/%06d.jpg'\n",
    "out_video = output + '.mp4'\n",
    "\n",
    "!ffmpeg -y -v warning -i $out_sequence $out_video\n",
    "\n",
    "Video(out_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\n",
    "Now let's generate custom animation. For that we omit model extension, so the script would load custom network, effectively enabling special features, e.g. arbitrary resolution (set by `--size` argument in `X-Y` format).  \n",
    "`--cubic` option changes linear interpolation to cubic for smoother animation (there is also `--gauss` option for additional smoothing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/_genSGAN2.py --model $model --out_dir $output --frames $frames --size 400-300 --cubic\n",
    "Image(output+'/000000.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  **Run `ffmpeg` command above after each generation, if you want to check results in motion.**  \n",
    "Adding `--save_lat` option will save all traversed dlatent points as Numpy array in `*.npy` file (useful for further curating). Set `--seed X` value to produce repeatable results (0 = random)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate more various imagery:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/_genSGAN2.py --model $model --out_dir $output --frames $frames --size 768-256 -n 3-1\n",
    "Image(output+'/000000.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here we get animated composition of 3 independent frames, blended together horizontally (similar to the image in the repo header). Argument `--splitfine X` controls boundary fineness (0 = smoothest/default, higher => thinner).  \n",
    "\n",
    "Instead of frame splitting, we can load external mask from b/w image file (it also could be folder with file sequence):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/_genSGAN2.py --model $model --out_dir $output --frames $frames --size 400-300 --latmask _in/mask.jpg\n",
    "Image(output+'/000000.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \n",
    "`--digress X` adds some funky displacements with X strength (by tweaking initial constant layer).  \n",
    "`--trunc X` controls truncation psi parameter (0 = boring, 1+ = weird). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/_genSGAN2.py --model $model --out_dir $output --frames $frames --digress 2 --trunc 1.2 --save_lat\n",
    "Image(output+'/000000.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Don't forget to check other options of `_genSGAN2.py` by `--help` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent space exploration\n",
    "\n",
    "For these experiments download [FFHQ model](https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-ffhq-config-f.pkl) and save to `models`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, Video\n",
    "\n",
    "model = 'models/stylegan2-ffhq-config-f' # without \".pkl\" extension\n",
    "model_pkl = model + '.pkl' # with \".pkl\" extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \n",
    "Project external images (aligned face portraits) from `_in/photo` onto StyleGAN2 model dlatent space. \n",
    "Results (found dlatent points as Numpy arrays in `*.npz` files, and video/still previews) are saved to `_out/proj` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/projector.py --model $model_pkl --in_dir _in/photo --out_dir _out/proj --save_video "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \n",
    "Generate animation between saved dlatent points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlat = 'dlats'\n",
    "path_in = '_in/' + dlat\n",
    "path_out = '_out/ffhq-' + dlat\n",
    "out_sequence = path_out + '/%06d.jpg'\n",
    "out_video = path_out + '.mp4'\n",
    "\n",
    "%run src/_play_dlatents.py --model $model --dlatents $path_in --out_dir $path_out --fstep 10\n",
    "Image(path_out+'/000000.jpg', width=512, height=512)\n",
    "\n",
    "!ffmpeg -y -v warning -i $out_sequence $out_video\n",
    "Video(out_video, width=512, height=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This loads saved dlatent points from `_in/dlats` and produces smooth looped animation between them (with interpolation step of 10 frames). `dlats` may be a file or a directory with `*.npy` or `*.npz` files. To select only few frames from a sequence `somename.npy`, create text file with comma-delimited frame numbers and save it as `somename.txt` in the same directory (check given examples for FFHQ model).\n",
    "\n",
    "Style-blending argument `--style_dlat blonde458.npy` would also load dlatent from `blonde458.npy` and apply it to higher network layers. `--cubic` smoothing and `--digress X` displacements are also applicable here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/_play_dlatents.py --model $model --dlatents $path_in --out_dir $path_out --fstep 10 --style_dlat _in/blonde458.npy --digress 2 --cubic\n",
    "!ffmpeg -y -v warning -i $out_sequence $out_video\n",
    "Video(out_video, width=512, height=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \n",
    "Generate animation by moving saved dlatent point `_in/blonde458.npy` along feature direction vectors from `_in/vectors_ffhq` (aging/smiling/etc) one by one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run src/_play_vectors.py --model $model_pkl --base_lat _in/blonde458.npy --vector_dir _in/vectors_ffhq --out_dir _out/ffhq_looks\n",
    "\n",
    "!ffmpeg -y -v warning -i _out/ffhq_looks/%06d.jpg _out/ffhq-vectors.mp4\n",
    "Video('_out/ffhq-vectors.mp4', width=512, height=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Such vectors can be discovered, for example, with the following methods:\n",
    "> * https://github.com/genforce/sefa\n",
    "> * https://github.com/harskish/ganspace\n",
    "> * https://github.com/LoreGoetschalckx/GANalyze"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "StyleGAN2.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
